<?xml version="1.0" encoding="ISO-8859-15"?>
<!DOCTYPE book
[
<!ENTITY % entities SYSTEM 'catalog.ent'>
<!ENTITY % local.common.attrib "infoid CDATA #IMPLIED">
<!ATTLIST bookinfo code CDATA #REQUIRED>
%entities;
]>



<book id="aims_training%book">
  <bookinfo code='aims_training'><title>Aims tutorial</title></bookinfo>


<chapter><title>Foreward</title>
&disclaimer_en;
<para>In order to work through the following sections, please download the demonstration data from one of the following links:
 <itemizedlist>
 <listitem><ulink url="ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data_td_2007.zip">ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data_td_2007.zip</ulink></listitem>
 <listitem><ulink url="ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data_td_2007.zip">ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data_td_2007.tar.gz</ulink></listitem>
 <listitem>Section <emphasis>Exemple data</emphasis> from <ulink url="http://brainvisa.info/downloadpage.html">http://brainvisa.info/downloadpage.html</ulink></listitem>
</itemizedlist>
</para>
<para>For more information concerning the installation, please refer to <ulink url="#bv_man%book"> the handbook of BrainVISA</ulink>.</para>
<para>In order to read the help of one commande line, either tape <emphasis> name_commmande -h</emphasis> or refer to the list of commande lines on <ulink url="$PATH_WEB;" target='blank'>http://brainvisa.info</ulink>.</para>
</chapter>


<!-->             <-->
<chapter>
<title>Basic command lines </title>

<sect1 id='aims_training%aimsfileconvert' infoid='conversion of file'>
<title>AimsFileConvert: Performs file format and data conversion</title>

<para><emphasis role='bold'>HELP:</emphasis> <ulink url='&PATH_WEB;#aims_AimsFileConvert' target='blank'>command help for AimsFileConvert</ulink></para>
<para><emphasis role='bold'>DATA:</emphasis> <filename>&DIR_ANAT;objects/3d/gis/subject01.ima</filename>.</para>
<para><emphasis role='bold'>EXAMPLE:</emphasis> here is the conversion from GIS to ANALYZE format.</para>
<screen>
prompt% AimsFileConvert -i subject01.ima -o subject01.img
</screen>
<para><emphasis role='bold'>NOTE:</emphasis> if you work with dicom files, all slices of the same acquisition must be located in the same directory.</para>
</sect1>


<sect1 id='aims_training%aimssubvolume' infoid='using of AimsSubVolume and application to diffusion data'>
<title>AimsSubVolume: Carve a subvolume in the input volume </title>

<para><emphasis role='bold'>HELP:</emphasis> <ulink url='&PATH_WEB;#aims_AimsSubVolume' target='blank'>command help for AimsSubVolume</ulink></para>

<para><emphasis role='bold'>DATA:</emphasis> <filename>&DIR_DEMO;AimsSubVolume/diff_data.ima</filename>, diffusion volume of size [256, 256, 23, 22].</para>

<para><emphasis role='bold'>EXAMPLE 1: GET T2 VOLUME FROM DIFFUSION DATA.</emphasis> In order to get the first volume of the 4th dimension, which corresponds to a T2 volume, you can use the AimsSubVolume command in this way:  </para>

<screen>
prompt%    
prompt% AimsSubVolume -i diff_data.ima -o t2.ima -t 0 -T 0
Input volume dimensions : 256 256 23 22
Output volume dimensions : 256 256 23 1
prompt%    
</screen>
<para>NOTE: concerning a volume of size [x, y, z, 3] has 3 time step, indices start at 0, so the time index must have a value between 0 and 2. This information can be read in the header file or into an anatomist browser.</para> 
 
<para><emphasis role='bold'>EXAMPLE 2: GET THE FIRST 3 DIFFUSION VOLUMES FROM THE DIFFUSION DATA.</emphasis> In order to get the first 3 volumes of diffusion data only<!--, you can do the following-->:</para>
 <screen>
prompt% AimsSubVolume -i diff_data.ima -o vol1.ima vol2.ima vol3.ima -t 1 2 3 -T 1 2 3
Input volume dimensions : 256 256 23 22
Output volume dimensions : 256 256 23 1
Input volume dimensions : 256 256 23 22
Output volume dimensions : 256 256 23 1
Input volume dimensions : 256 256 23 22
Output volume dimensions : 256 256 23 1
prompt%
</screen>
  
<para><emphasis role='bold'>EXAMPLE 3: GET ALL DIFFUSION VOLUMES FROM DIFFUSION DATA.</emphasis> In order to remove the T2 data from diffusion data and to keep only the diffusion data<!--, you can do the following-->: </para>
<screen>
prompt%% AimsSubVolume -i diff_data.ima -o diff.ima -t 1 -T 21
Input volume dimensions : 256 256 23 22
Output volume dimensions : 256 256 23 21
prompt%
</screen>  

</sect1>
  
  
<sect1 id='aims_training%aimsthreshold' infoid='using of AimsThreshold and application to diffusion data'><title>AimsThreshold: Threshold on data</title>

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsThreshold' target='blank'>command help for AimsThreshold</ulink></para>

<para><emphasis role='bold'>DATA:</emphasis> <filename>&DIR_DEMO;AimsThreshold/voronoi_subject01.ima</filename>.</para>

<para><emphasis role='bold'>EXAMPLE: select a label.</emphasis> For instance, your image is a label volume with 4 values: label 0 = background, label 1 = one hemisphere, label 2 = second hemisphere and label 3 = cerebellum. If you want to remove the cerebellum, you can set up a threshold to keep all values lower than 3:</para>
<screen>
prompt% AimsThreshold -i voronoi_lesson1.ima -o hemi_only.ima -m lt -t 3
</screen>

<figure><title>Select label</title>
<mediaobject>
	<imageobject role="fop"><imagedata fileref="&DIR_IMG;aimsthreshold.png" format="PNG"  width="150"/></imageobject>
	<imageobject role="html"><imagedata fileref="&DIR_IMG;aimsthreshold.png" format="PNG"/></imageobject>
</mediaobject>
</figure>


</sect1> 



<sect1 id='aims_training%aimsgraphmesh' infoid='using of AimsGraphMesh'>
<title>AimsGraphMesh: Performs graph storage conversion and sub-buckets meshing. This command is an improved version of AimsGraphConvert</title>

<para><emphasis role='bold'>HELP:</emphasis> <ulink url='&PATH_WEB;#aims_AimsGraphMesh' target='blank'>command help for AimsGraphMesh</ulink></para>

<para><emphasis role='bold'>DATA:</emphasis> <filename>&DIR_ANAT;objects/roi/basal_ganglia.arg</filename> and <filename>&DIR_ANAT;objects/roi/basal_ganglia.dim</filename>.</para>
	
<para><emphasis role='bold'>EXAMPLE: mesh a ROI graph.</emphasis> The viewing will be enhanced if the ROI graph is meshed.</para>
<screen>
prompt%  AimsGraphMesh -i basal_ganglia.arg -o mesh_basal_ganglia.arg
Warning: wrong filename_base in graph, trying to fix it
filename_base : mesh_basal_ganglia.data
bound : (121 ,127 ,66)
reading slice      :  67
getting interface  : done
processing mesh    : done
clearing interface : done
bound : (151 ,153 ,66)
reading slice      :  67
getting interface  : done
processing mesh    : done
clearing interface : done
bound : (153 ,137 ,71)
reading slice      :  72
getting interface  : done
processing mesh    : done
clearing interface : done
....
saving all
</screen>

<figure><title>Viewing of non-meshed and meshed ROI</title>
<mediaobject>
	<imageobject role="fop"><imagedata fileref="&DIR_IMG;aimsgraphmesh.png" format="PNG"  width="150"/></imageobject>
	<imageobject role="html"><imagedata fileref="&DIR_IMG;aimsgraphmesh.png" format="PNG"/></imageobject>
</mediaobject>
</figure>


</sect1> 



<sect1 id='aims_training%aimsroifeatures' infoid='using of AimsRoiFeatures'>
<title>AimsRoiFeatures: Compute scalar features (mean, volume ...) from regions of interest.</title>

<para><emphasis role='bold'>HELP:</emphasis> <ulink url='&PATH_WEB;#aims_AimsRoiFeatures' target='blank'>command help for AimsRoiFeatures</ulink></para>
	
<para><emphasis role='bold'>DATA:</emphasis> <filename>&DIR_ANAT;objects/roi/anat_demo_roi.ima</filename> and <filename>&DATA_DEMO;AimsRoiFeatures/masque_thalamus_gauche.ima</filename></para>

<para>Here is an example using a binary mask (so all voxels are set to <emphasis>1</emphasis>, in other words there is a label called <emphasis>1</emphasis>) and a volume: </para>
<screen>
prompt% AimsRoiFeatures -i masque_thalamus_gauche.ima --imageStatistics 1:anat_demo_roi.ima -o roi_features.txt
prompt% more features.txt
attributes = {
'format': 'features_1.0',
'content_type': 'roi_features',
'1': {
'point_count': 6502,
'volume': 6857.58,
'1': {
'mean': 48.797,
'stddev': 8.32696,
'min': 21.0003,
'max': 69.9999,
'median': 50,
},
},
}
</screen>

</sect1> 







<!-- 
<sect1 id='aims_training%aimsmasscenter' infoid=''>
<title>AimsMassCenter: Computes position of the mass center of the image</title>
  
<sect2><title>Options</title>  
<para>
<screen>
Computes position of the mass center of the image

Options 

-i | input &lt;file name (read only): { Volume of FLOAT, Volume of S16, Volume
    of S8, Volume of U8 }&gt;  input data

[ -b |  binary &lt;boolean&gt; ]
    consider input image as binary data

[  verbose [ &lt;S32&gt; ] ]
    Set verbosity level (default = 0, without value = 1)

[ -h |  help &lt;boolean&gt; ]
    show help message

[  version &lt;boolean&gt; ]
    show Cartograph version

[  info &lt;boolean&gt; ]
    show libraries information (install, plugins, paths, etc.)

[  optionsfile &lt;string&gt; ]
    Read additional commandline options from the specified file (one switch or
    value per line)

</screen>
</para>
</sect2>
  
<sect2><title>Example: </title>  
  <para> </para>
  <screen>
prompt%    
prompt%
prompt%
</screen>
<para><emphasis rol="bold">What means mass and vol ?</emphasis></para>
<para>Your volume is composed by voxels, which have a grey level, a label or a value (0/1 for binary volume) and size in x, y and z.
Then several things can be computed:</para> 
<para>Voxel_volume = sizeX * sizeY * sizeZ in cm3</para>
<para>Mass = Voxel_volume * sum of voxel value</para>
<para>Volume = Voxel_volume * number of voxels </para>
<para></para>
<para>This command is used for PET data, this data are composed with a temporal dimension. Thus in this command, if your data are n 
frame then you will have n lines plus a "General" line otherwise you will obtain "0" line and the same with "General" line.</para>  
</sect2>

</sect1>   
-->
<!--
<sect1 id='aims_training%aimsmasscenter' infoid=''>
<title>AimsMassCenter: </title>
  
<sect2><title>Options</title>  
<para>
<screen>

</screen>
</para>
</sect2>
  
<sect2><title>Example: </title>  
  <para> </para>
  <screen>
prompt%    
prompt%
prompt%
</screen>
<para></para> 
</sect2>

</sect1>   
-->

</chapter>  


<!--> <-->
<chapter>
<title>Conversion</title>

<sect1 id='aims_training%aimsdiffusionbundletoroi' infoid='usage of AimsDiffusionBundleToRoi'>
<title>AimsDiffusionBundleToRoi: conversion from bundles.bundles to ROI graph</title> 

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsDiffusionBundleToRoi' target='blank'>command help for AimsDiffusionBundleToRoi</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<para><emphasis role='bold'>EXAMPLE: </emphasis></para> 
<screen>prompt% AimsDiffusionBundleAnalysis -i my_bundles.bundles -g my_bundles.arg</screen>

</sect1> 


<sect1 id='aims_training%aimsgraphconvert' infoid='usage of AimsGraphConvert'>
<title>AimsGraphConvert: conversion from label image to ROI graph</title> 

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsGraphConvert' target='blank'>command help for AimsGraphConvert</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<para><emphasis role='bold'>EXAMPLE 1: </emphasis></para> 
<screen>prompt% AimsGraphConvert -i label_image.ima -o label_graphe.arg --bucket</screen>
<para><emphasis role='bold'>EXAMPLE 2: </emphasis>Mesh the graph.</para> 
<screen>prompt% AimsGraphMesh -i label_graphe.arg -o m_label_graphe.arg</screen>

<!-- <listitem>Optionally, you can use a nomenclature to link a region name with a specific color. Please see the <emphasis><ulink url="#ana_training%write_nomenclature">"Write a simple nomenclature (.hie)"</ulink></emphasis> section of <emphasis>The Anatomist getting started guide</emphasis>.</listitem>
</itemizedlist> 
-->
</sect1> 


<sect1 id='aims_training%aimsconversion' infoid='table of format conversions'>
<title>Table of format conversions</title> 

<para>Here are some very useful command lines to convert data. However, all command options are not explained in details. Please refer to the command help.</para>
<table>
<title>Table of format conversions</title>
<tgroup cols="4">
<thead>
<row>
<entry>Input (format/type)</entry>
<entry>Output (format/type)</entry>
<entry>commande line</entry>
<entry>Note</entry>
</row>
</thead>
<tbody>
<row>
<entry>GIS</entry><entry>MINC</entry>
<entry><screen>prompt% AimsFileConvert my_volume.ima my_volume.mnc</screen></entry><entry></entry>
</row>
<row>
<entry>.bundles</entry><entry>.arg</entry>
<entry><screen>prompt% AimsDiffusionBundleAnalysis -i my_bundles.bundles -g my_bundles.arg</screen></entry><entry></entry>
</row>
<row>
<entry>Label_image (volume)</entry><entry>.arg</entry>
<entry><screen>prompt% AimsGraphConvert -i label_image.ima -o label_graphe.arg --bucket</screen></entry><entry></entry>
</row>
<row> 
<entry>.arg</entry><entry>Label_image (volume)</entry>
<entry><screen>prompt% AimsGraphConvert -i roi.arg -o roi.arg --volume </screen></entry><entry>You will find roi_Volume.ima in roi.data directory</entry>
</row>
<row>
<entry>Volume</entry><entry>Cluster graph</entry>
<entry><screen>prompt% AimsClusterArg -i volume.ima -o cluster.arg</screen></entry><entry></entry>
</row>
<row>
<entry>Volume</entry><entry>Mesh</entry>
<entry><screen>prompt% AimsMesh -i label_image.ima -o label_image.mesh</screen></entry><entry></entry>
</row>
<row>
<entry>Mesh</entry><entry>Ascii</entry>
<entry><screen>prompt% AimsMesh2Ascii input.mesh mesh.txt</screen></entry><entry></entry>
</row>
<row>
<entry>Binary image</entry><entry>Mesh</entry>
<entry><screen>prompt% AimsMesh -i binary_mask.ima -o mask.mesh </screen></entry><entry>The output file will be mask_1_0.mesh</entry>
</row>
</tbody>
</tgroup>
</table>
</sect1>

</chapter> 






<!-->  <-->
<chapter>
<title>Calculation of images</title>

<sect1 id='aims_training%aimslinearcomb' infoid='using of AimsLinearComb'><title>AimsLinearComb: sum 2 activation maps</title>

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsLinearComb' target='blank'>command help for AimsLinearComb</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<para><emphasis role='bold'>EXAMPLE: Sum of 2 activation maps.</emphasis> For instance, if you have 2 binary activation maps which have been obtained by functional analysis, and if you want to do a fusion of both, then you can create a new volume which will be the sum of map_I and map_II.</para>
<screen>
prompt% AimsLinearComb -i map_I.img -j map_II.img -o map_I+II.img
	options parsed
	i1      : map_I.img
	a       : 1
	b       : 1
	c       : 1
	d       : 1
	e       : 0
	i2      : map_II.img
	fileout : map_I+II.img
	type    :
	Reading image map_I.img...
	done
	reading image map_II.img...
	done
	processing...
	done
	writing result...
	done
</screen>

<figure><title>Sum of 2 activation maps</title><mediaobject>	
	<imageobject role="fop"><imagedata fileref="&DIR_IMG;aimslinearcomb.png" format="PNG"  width="150"/></imageobject>
	<imageobject role="html"><imagedata fileref="&DIR_IMG;aimslinearcomb.png" format="PNG"/></imageobject>
</mediaobject>
</figure>
<para>NOTE: image dimensions must be the same.</para>
<para>NOTE: you can use this command line to add several volumes; first add up map_I and map_II to create map_I+II and if you have a third volume, map_III to fusion with map_I and map_II, then you can use AimsLinearComb with map_I+II and map_III.</para>
<para>NOTE: advanced use, you can use multiplicative and divisor coefficients for each volume.</para>
</sect1>

<!-- 
<sect1 id='aims_training%cartoLinearComb' infoid='cartoLinearComb'>
<title>cartoLinearComb: Apply a formula to a set of homogeneous images (homogeneous means all of the same data type)</title> 
 <para>
Options:
  -h, - -help            show this help message and exit
  -o OUTPUT, - -output=OUTPUT
                        output volume
  -f FORMULA, - -formula=FORMULA
                        image formula, ex: ( I1 * 2 + I2 * I3 ) / 1.2
  -i FILENAME, - -input=FILENAME
                        input volume(s)
</para>


<para><emphasis role='bold'>EXAMPLE: </emphasis></para> 
<screen>prompt% cartoLinearComb -i image1.ima image2.ima -o image1+image2.ima -f I1 + I2</screen>
<para>NOTE: it is a beta version with some tricks, for example be aware that data don't have values to zero when
you want to do a division ....</para>
</sect1> 
-->


</chapter> 


<!--> <-->
<chapter>
<title>Handling meshes</title>

<sect1 id='aims_training%AimsConvexHull' infoid=''>
<title>Creation of a cube mesh from a point list</title> 

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsConvexHull' target='blank'>command help for AimsConvexHull</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<para><emphasis role='bold'>EXAMPLE: </emphasis>
<itemizedlist>
<listitem><para>Write a the following text file and save under cube.txt:</para>
<literallayout>BEGIN------CUT HERE------BEGIN
8
0 0 0
10 0 0
0 10 0
10 10 0
0 0 10
10 0 10
0 10 10
10 10 10
END------CUT HERE------END
</literallayout></listitem>
<listitem><screen>prompt% AimsConvexHull -i cube.txt -o cube.mesh </screen></listitem>
</itemizedlist>
</para>
</sect1> 

<sect1 id='aims_training%aimszcat' infoid='using of AimsZCat'>
<title>AimsZCat: concatenates volumes (along Z axis), meshes or buckets</title>

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsZCat' target='blank'>command help for AimsZCat</ulink></para>
	
<para><emphasis role='bold'>DATA: </emphasis><itemizedlist>
<listitem><filename>&DIR_BV;demo/subject01/tri/subject01_Lhemi.mesh</filename></listitem>
<listitem><filename>&DIR_BV;demo/subject01/tri/subject01_Rhemi.mesh</filename></listitem>				</itemizedlist></para>

<para><emphasis role='bold'>EXAMPLE: concatenation both right and left hemisphere meshes</emphasis></para> 
<screen>prompt% AimsZCat -i subject01_Lhemi.mesh subject01_Rhemi.mesh -o right_and_left_hemisphere.mesh</screen>
	
</sect1> 


</chapter> 



<!--> <-->
<chapter>
<title>Handling referentials and transformations</title>

<sect1 id="aims_training%referentials" infoid="referentials">
  <title>Coordinates systems in AIMS</title>

  <para>Here is a description of the coordinates sytems used in Aims and Anatomist, and what I have understood of how SPM handles its referentials.
  </para>

  <sect2>
    <title>AIMS and Anatomist</title>
    <sect3><title>Internally</title>
      <para>
        Anatomist uses AIMS to handle its referentials so behaves exactly the same way.
      </para>
      <para>Aims tries to work internally in an image-specific referential, but with always the same orientation. This orientation is axial with the following coordinates system:
        <itemizedlist>
          <listitem>X axis: right to left</listitem>
          <listitem>Y axis: front to back</listitem>
          <listitem>Z axis: top to bottom</listitem>
          <listitem>origin: the center of the <emphasis>first</emphasis> voxel: the voxel in the top, right, front corner
          </listitem>
        </itemizedlist>
        If you look at it you will realize that this referential is in <emphasis>radiological</emphasis> convention and is <emphasis>indirect</emphasis>. This is, in my opinion, a bad choice, but it's a bit too late to change.
      </para>
      <para>Once loaded in memory, all voxels should be organized in this order. As a consequence, images in Anatomist are always displayed in radiological mode, whatever the actual orientation of data on disk.
      </para>
    </sect3>

    <sect3><title>Externally</title>
      <para>
        Images on disk, depending on their format and acquisition modes, are not necessarily in this orientation. When a different orientation is detected, images are flipped in memory at load-time to fit the standard AIMS orientation. And when images are written back to disk, they may also be flipped back according to the specific format needs.
      </para>
    </sect3>

    <sect3><title>Transformations</title>
      <para>
        By default, AIMS doesn't apply any transformation other than flipping images at load time as described just before.
      </para>
      <para>
        But transformations can be provided in some Aims commands or loaded in Anatomist to apply coordinates changes. Then coords transformations are applied on the fly when processing or displaying data which are not in the same referential.
      </para>
      <para>There is no special referential (such as a common central working referential).
      </para>
      <para>
        Transformation files used by AIMS (<computeroutput>.trm</computeroutput> files) are ASCII files looking like this:
        <screen>
Tx Ty Tz
R11 R12 R13
R21 R22 R23
R31 R32 R33</screen>
        Tx, Ty, Tz are the translation components while the Rij coefficients are the linear matrix part. When used, these coefficients are applied as a "standard" 4x4 transformation matrix:
        <screen>
    [ R11  R12  R13  Tx ]
M = [ R21  R22  R23  Ty ]
    [ R31  R32  R33  Tz ]
    [   0    0    0   1 ]</screen>
      </para>
    </sect3>

    <sect3 id="aims_training%minf">
      <title><computeroutput>MINF</computeroutput> files</title>
      <para>
        AIMS (and Anatomist) writes an additional header file which can store any additional information: the <computeroutput>.minf</computeroutput> header (for Meta-INFormation) when saving its data (images, meshes, and any other data), and reads it if it is present when loading data files. This meta-header has the shape displayed by the <computeroutput>AimsFileInfo</computeroutput> command, and may be saved in "python dictionary" or XML formats. The MINF file has the same file name as the main data file, with the <computeroutput>.minf</computeroutput> extension added (<computeroutput>toto.img.minf</computeroutput> for instance).
      </para>
      <para>
        The MINF header may contain referentials and transformations information. When present, this information is stored in a few fields:
        <itemizedlist>
          <listitem>
            <emphasis role="bold"><computeroutput>referential</computeroutput></emphasis> may store an unique identifier (a cryptic characters string) to identify the AIMS referential for the current data file. If several data files refer to the same identifier, then they share the same referential and are considered to have coordinates in the same system.
          </listitem>
          <listitem>
            <emphasis role="bold"><computeroutput>referentials</computeroutput></emphasis> may store a list of target referentials for transformations specified in the <computeroutput>transformations</computeroutput> field. Both fields must have the same number of entries. Referentials are identified by character strings, either as unique identifiers or generic names (not necessarily unique). Some standard common referentials have specific names: "<computeroutput>Talairach-MNI template-SPM</computeroutput>" for the MNI normalization referential (used by SPM for instance), or "<computeroutput>Talairach-AC/PC-Anatomist</computeroutput>" for the referential based on anterior and posterior commissures used by the BrainVISA anatomical segmentation pipeline.
          </listitem>
          <listitem>
            <emphasis role="bold"><computeroutput>transformations</computeroutput></emphasis> may store a list of transformation matrices, each going from the AIMS data referential to the corresponding referential specified in the <computeroutput>referentials</computeroutput> field (same position in the list). Each transformation is a 4x4 matrix written as 16 numbers in rows, and assumes all coordinates are in millimeters.
          </listitem>
          <listitem>
            <emphasis role="bold"><computeroutput>storage_to_memory</computeroutput></emphasis> may store the disk orientation information, by providing the transformation between the disk storage voxels order and the memory orientation (the AIMS referential). This transformation is in the same shape as the <computeroutput>transformations</computeroutput> field, except that it is not a list, and the transformation is in voxels, not in mm.
          </listitem>
        </itemizedlist>
        For instance a MINF file may look like the following (in "python dictionary" format, here):
<screen>attributes = {
  'storage_to_memory' : [ 1, 0, 0, 0, 0, -1, 0, 62, 0, 0, -1, 45, 0, 0, 0, 1 ],
  'referentials' : [ 'Coordinates aligned to another file or to anatomical truth' ],
  'transformations' : [ [ -1, 0, 0, 78, 0, -1, 0, 75, 0, 0, -1, 84, 0, 0, 0, 1 ] ],
  'referential': 'be9724cc-eceb-d831-a83e-335e12b80f14',
}</screen>
        The referentials and transformations information in the MINF header may reflect information already stored in the specific format header (Analyze origin, or NIFTI-1 qform and sform, or MINC transformation).
      </para>
    </sect3>
  </sect2>

  <sect2>
    <title>SPM</title>

    <sect3><title>Internally</title>
      <para>
        Internally, SPM <emphasis>thinks</emphasis> things are always in the same orientation, which is also axial but with different axes:
        <itemizedlist>
          <listitem>X axis: left to right</listitem>
          <listitem>Y axis: back to front</listitem>
          <listitem>Z axis: bottom to top</listitem>
          <listitem>origin: the center of the voxel specified by the <emphasis>origin</emphasis> field of the SPM image header. This origin is specified in voxels and starts counting from 1 (not 0) like a matlab array index does.
          </listitem>
        </itemizedlist>
        This is a <emphasis>neurological</emphasis> convention orientation. The axes happen to be exactly the contrary of what is done in AIMS. Bad luck... But this referential is direct so cannot be considered worse than in AIMS...
      </para>
      <para>
        Working on the coordinate transformations for years and regularly getting headaches from it, I am still not 100% sure of what I say here, so if I'm wrong, please correct me by sending a message on BrainVisa forum (<ulink url="http://brainvisa.info/forum/">http://brainvisa.info/forum/</ulink>). Especially, I'm not sure that SPM99 and SPM2 really use the same referentials.
      </para>
    </sect3>

    <sect3><title>Externally</title>
      <para>
        SPM handles input Analyze images in two different orientations: axial radiological and axial neurological orientations. This orientation is <emphasis role="bold">not specified</emphasis> in SPM-Analyze format image files, so <emphasis role="bold">you</emphasis> have to tell how they are oriented. This is done in SPM by a flipping flag set somewhere in SPM defaults configuration (<computeroutput>default.analyze.flip</computeroutput> in SPM2).
      </para>
      <para>
        This is specific to SPM-Analyze format, and does not apply to NIFTI-1 or Minc formats. Hopefully the Analyze format is now obsolete and will disapear with time, but there are still existing files...
      </para>
      <para>This flipping flag has changed in form and meaning between SPM99 and SPM2.
      </para>
      <para>
        As I have understood:
        <itemizedlist>
          <listitem><emphasis role="bold">SPM99:</emphasis>
            <itemizedlist>
              <listitem>
                SPM99 uses the flipping flag only when normalizing images, indicating that the normalization process must perform or not a flip towards the normalization template. A clear indication of it is that the flag is part of the normalization parameters and is not present in other parts of SPM.
              </listitem>
              <listitem>
                Otherwise, SPM99 does not bother about the orientation of images. This is to say: even when displaying images, radiological images will be displayed with the left on the right of the display window, and neurological images with the left on the left, regardless of the flipping flag.
              </listitem>
              <listitem>
                Normalized images are <emphasis role="bold">always</emphasis> in neurological orientation whatever the orientation of input unnormalized images. Consequently, after normalizing a radiological image, loading both a normalized image and an unnormalized one in SPM will display them with different orientations.
              </listitem>
              <listitem>
                I am not sure if normalization templates have to be necessarily in neurological orientation or not but I guess yes because there is no way to indicate that the template is in radiological orientation.
              </listitem>
              <listitem>
                Normalization matrices for radiological data contain a X axis flip (negative 1st coefficient)
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem><emphasis role="bold">SPM2:</emphasis>
            <itemizedlist>
              <listitem>
                SPM2 uses the flipping flag at load time: radiological images are systematically flipped when loaded (and flipped back when rewritten so as to keep their radiological orientation on disk). This is true for <emphasis role="bold">all Analyze/SPM</emphasis> image files.
              </listitem>
              <listitem>
                This means all processings use it. As a consequence, all images are displayed in neurological orientation, left on the left, even for radiological images.
              </listitem>
              <listitem>
                But as this flag is global in SPM, <emphasis>all</emphasis> SPM images are considered to be in the same orientation: you cannot mix radio and neuro images. What I am pointing out here is only valid for SPM format images: SPM2 also handles MINC format, and Minc images contain orientation information.
              </listitem>
              <listitem>
                Normalized images are now in the same orientation as the input unnormalized image. Normalizing a radiological image will result in a normalized file in the radiological orientation (in SPM format). <emphasis role="bold">This is not what SPM99 used to do</emphasis>.
              </listitem>
              <listitem>
                SPM2 does not understand SPM99 and vice versa: no compatibility at all (neither forward nor backward): if you are using the radiological convention (like we are), loading in SPM2 an image normalized by SPM99 will result in a spurious flip and incorrect display and processing. This means you cannot use with SPM2 an image database built with SPM99.
              </listitem>
              <listitem>
                Normalization templates can be in either orientation. More precisely, I guess the template must be in the orientation specified by the flipping flag, or in Minc format in neurological orientation. I'm not completely sure of this. But this is perhaps an explanation of why the standard normalization template is now in Minc format and not in SPM format (otherwise its interpretation would depend on a user-defined flag).
              </listitem>
              <listitem>
                <para>
                  Last minute: I have just discovered that SPM now sometimes produces images with negative voxel sizes. I guess it is a kind of flipping indication, but I don't know from what to what else. And we know that all radiological images don't have this negative voxel size feature. So my opinion is that it's not reliable at all (at least unless you exactly know which version of SPM has written each image and this info is not available). This sign information is ignored in the current version of AIMS.
                </para>
                <para>The headache goes on...</para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem><emphasis role="bold">SPM5 ans SPM8:</emphasis>
            <itemizedlist>
              <listitem>
                SPM5 now uses the NIFTI-1 format for all output. NIFTI-1 specifies orientations and possibly transformations to standard referentials in its format, so this is a very good thing. Many problems are now solved.
              </listitem>
              <listitem>
                Otherwise I guess SPM5 behaves essentially like SPM2.
              </listitem>
              <listitem>
                The only little imperfection is that when SPM5 performs normalizations towards the MNI template, it does not indicate in output image that the target referential is the MNI template, but an unspecified other referential instead. So <emphasis role="bold">you</emphasis> have to know the target referential and specify it when needed (for instance in Anatomist).
              </listitem>
            </itemizedlist>
          </listitem>
        </itemizedlist>
      </para>
    </sect3>

    <sect3><title>Transformations</title>
      <para>
        SPM uses a common central referential to work in. Every image can provide a transformation matrix to this referential. Such a transformation may be specified in different ways:
        <itemizedlist>
          <listitem>an optional <computeroutput>.mat</computeroutput> file with the same name as the SPM format image. this was the way SPM99 and SPM2 behaved with Analyze format. But with SPM5 and SPM8, using NIFTI-1 formats avoids this need.
          </listitem>
          <listitem>If this <computeroutput>.mat</computeroutput> is not provided, then the file format header information is considered. NIFTI-1 provides full affine transformation matrices, but Analyze has only the origin translation, which is considered to be the only transformation needed to reach the central referential. If the .mat file is specified, information contained in it overrides some of the header information (including the origin).
          </listitem>
        </itemizedlist>
      </para>
      <para>
        Normalization files (<computeroutput>*_sn3d.mat</computeroutput> for SPM99, <computeroutput>*_sn.mat</computeroutput> for SPM2 and newer) contain transformations to the referential of a normalization template (either a standard one provided with the SPM software distribution, or a custom user-made one). This transformation contains an affine part (matrix), and optionally, depending on the normalization type, a non-linear part (coefficients on a functions base as far as I know but I don't know much about this part). Information about the input and template images are also included (dimensions, and origins or voxels-to-template transformation).
      </para>
      <para>
        Normalized images are in the referential of the normalization template used, but not necessarily with the same bounding box, resolution and field of view.
      </para>
      <para>
        SPM99 and SPM2 use normalization files with different names and different contents. They are not compatible, even if there is some common and similar information in them.
      </para>
    </sect3>
  </sect2>

  <sect2>
    <title>Changing between SPM and AIMS</title>

    <para>
      Due to the different internal orientations of the coordinate systems, going from SPM to AIMS and vice versa causes some serious problems.
    </para>

    <sect3><title>Normalization</title>
      <para>SPM normalization files are in matlab (<computeroutput>.mat</computeroutput>) format. AIMS cannot read the proprietary matlab format, so such files cannot be directly imported in AIMS.
      </para>
      <para>
        However, the scipy module for Python language can read them. So we have made Python scripts in PyAims and in BrainVisa to convert SPM matrices to AIMS <computeroutput>.trm</computeroutput> format. <emphasis role="bold">Only the affine part can be converted</emphasis>, because AIMS only use matrices for transformations, and non-linear information cannot fit into a matrix. Look at the <computeroutput>AimsSpmNormalizationConvert.py</computeroutput> program, and the <computeroutput>SPMsn3dToAims</computeroutput> process in BrainVISA.
      </para>
      <para>
        As the orientation is different in SPM and AIMS, a transformation to a template image is not the same as a transformation to a normalized image with a different field of view. So, when converting SPM normalization matrices, the normalized image must be also provided, otherwise BrainVisa can only give the transformation to the normalization template. Note the difference.
      </para>
    </sect3>
  </sect2>

  <sect2>
    <title>Issues</title>

    <sect3><title>Unnormalized SPM/Analyze images</title>
      <para>
        It is impossible to guess the orientation of such images if you don't know how they were acquired. This means you have to manually specify their orientation, either for all images in SPM, or in BrainVisa when importing them into a database. BrainVisa tags them so it knows everything afterwards and avoids mistakes. SPM does not.
      </para>
    </sect3>

    <sect3><title>Normalized SPM/Analyze images</title>
      <para>
        Normalizing the same image in radiological orientation with SPM99 and SPM2 results in normalized images in different orientations. When you import normalized images coming from another site, you have to know whether they have been normalized by SPM99 or by SPM2, and if the original image was in radiological or neurological orientation.
      </para>
      <para>
        I think the normalization file (<computeroutput>*_sn3d.mat</computeroutput> for SPM99, or <computeroutput>*_sn.mat</computeroutput> for SPM2) contains enough information to retreive the orientation of input and template images, so can disambiguate the situation.
      </para>
    </sect3>

    <sect3><title>Reading SPM/Analyze origin</title>
      <para>
        The origin field of SPM format is the position of the referential origin, in voxels and starting from 1 (not from 0). In fact it's a matlab array index. So it is given in the orientation of the image on disk. AIMS flips SPM images on several axes when loading them, so the origin information also has to be flipped. Flipping it needs to know the image dimensions.
      </para>
      <para>
        AIMS referentials have their origin in the first voxel, (almost) in the corner of the image, and normally don't use the SPM origin. But the origin information is read and maintained. Anatomist can, if asked for, make a transformation going from AIMS origin (corner) to the SPM origin. This allows to display several aligned SPM images in Anatomist with the correct correspondance. However after this translation, the coordinates are still in AIMS orientation (radiological and indirect), not in SPM, so the coordinates do not correspond to what they are in SPM.
      </para>
      <para>
        To compare coordinates of SPM images in Anatomist and SPM, another transformation has to be applied in Anatomist, with all the flips included. Anatomist can directly use the SPM/MNI normalization referential.
      </para>
    </sect3>

    <sect3><title>Other formats (GIS etc)</title>
      <para>
        Up to now, GIS images are considered being always in AIMS orientation unless specified in their AIMS meta-header (<computeroutput>.minf</computeroutput> file, see <link linkend="aims_training%minf">the corresponding paragraph</link>). No flips are applied.
      </para>
      <para>
        The Minc and NIFTI-1 IO plugins take orientation into account and flip data accordingly when reading / writing files.
      </para>
      <para>
        I am not sure if other formats (Dicom, Ecat...) can specify an image orientation or not. If they do, the current release of AIMS will probably not take it into account.
      </para>
    </sect3>
  </sect2>


  <sect2>
    <title>Technical details</title>

    <sect3><title>SPM normalization matrices conversion to AIMS world</title>
      <para>
        SPM99 and SPM2 don't use the same format of normalization files, but both provide more or less the following information:
        <itemizedlist>
          <listitem>
            An affine transformation matrix, called <computeroutput>Affine</computeroutput>, transforming coordinates from the template space to the input space, both sides in voxels arrays index, and indexed from 1 (not from 0)
          </listitem>
          <listitem>
            A voxels-to-mm transformation matrix for the input image, transforming voxels of the image into a mm position in the SPM internal orientation, taking the origin into account, and possibly rotations if the format supports it (NIFTI). This matrix is called <computeroutput>VF.mat</computeroutput> in SPM2 and also performs flipping, and called <computeroutput>MF</computeroutput> in SPM99 but doesn't seem to contain the flipping information. However for nomrmalization this millimetric referential is quite undefined and we will not really use it.
          </listitem>
          <listitem>
            Another voxel-to-mm matrix for the template image: <computeroutput>VG.mat</computeroutput> in SPM2, or <computeroutput>MG</computeroutput> in SPM99.
          </listitem>
          <listitem>
            Input and template image dimensions in voxels and a bit more: <computeroutput>VF.dim</computeroutput> and <computeroutput>VG.dim</computeroutput> in SPM2, or <computeroutput>Dims</computeroutput> in SPM99.
          </listitem>
        </itemizedlist>
      </para>
    </sect3>

    <sect3><title>Notations:</title>
      <para>
        <itemizedlist>
          <listitem>
            3 images: input (I call it Anatomy to be clearer), template, and normalized images. I use the suffixes A, T and N for coordinates on these 3 images.
          </listitem>
          <listitem>
            I use the same name for a given referential and coordinates in this referential: for instance <computeroutput>RAA</computeroutput> is both the AIMS referential of the anatomical image and a coordinates vector in it. I don't bother about standardized math notations: I don't remember them and haven't been using math anymore for many years. Don't ask me too much.
          </listitem>
          <listitem>
            AIMS referentials:
            <itemizedlist>
              <listitem>
                <computeroutput>RAA</computeroutput>: anatomy (in mm, radio convention, origin in 1st voxel)
              </listitem>
              <listitem>
                <computeroutput>RAAv</computeroutput>: anatomy (in voxels, radio convention, origin in 1st voxel)
              </listitem>
              <listitem>
                <computeroutput>RAAd</computeroutput>: anatomy (in voxels, disk storage ordering)
              </listitem>
              <listitem>
                <computeroutput>RAN</computeroutput>: normalized (in mm...)
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
            SPM referentials:
            <itemizedlist>
              <listitem>
                <computeroutput>RSA</computeroutput>: anatomy, in voxels
              </listitem>
              <listitem>
                <computeroutput>RST</computeroutput>: template, in voxels
              </listitem>
              <listitem>
                <computeroutput>RSCT</computeroutput>: template, "central" in mm
              </listitem>
              <listitem>
                <computeroutput>RSCN</computeroutput>: normalized, "central" in mm. Actually, <computeroutput>RSCT</computeroutput> and <computeroutput>RSCN</computeroutput> are the same.
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>Transformation matrices:
            <itemizedlist>
              <listitem>
                <computeroutput>Affine</computeroutput>: the SPM affine matrix (voxels): <computeroutput>RST</computeroutput> to <computeroutput>RSA</computeroutput>
              </listitem>
              <listitem>
                <computeroutput>At</computeroutput>: SPM voxels to AIMS voxels transformation. This is only to take the array indexing starting at 1 in Matlab. So <computeroutput>At</computeroutput> is a <computeroutput>( -1, -1, -1 )</computeroutput> voxel translation. It can be used between <computeroutput>RSA</computeroutput> and <computeroutput>RAAd</computeroutput>, and either between <computeroutput>RSN</computeroutput> and <computeroutput>RANd</computeroutput>.
              </listitem>
              <listitem>
                <computeroutput>AIMS</computeroutput>: <computeroutput>RAA</computeroutput> to <computeroutput>RAN</computeroutput>, what we want to calculate. Here again, I'm maybe not using correctly math notations. I mean: <computeroutput>RAN = AIMS * RAA</computeroutput>.
              </listitem>
              <listitem>
                <computeroutput>VsA</computeroutput>: Aims voxels to mm anat
              </listitem>
              <listitem>
                <computeroutput>S2MA</computeroutput> Aims "<computeroutput>storage_to_memory</computeroutput>" anat matrix: disk voxels to Aims voxels.
              </listitem>
              <listitem>
                <computeroutput>A2T</computeroutput>: Aims anat-mm to template space-mm: <computeroutput>RAA</computeroutput> to <computeroutput>RSCT</computeroutput>, what we want to calculate if no normalized image is used.
              </listitem>
              <listitem>
                <computeroutput>TN</computeroutput>: normalized, Aims-mm to SPM-central-mm: <computeroutput>RAN</computeroutput> to <computeroutput>RSCN</computeroutput>
              </listitem>
              <listitem>
                <computeroutput>TCN</computeroutput>: template to normalized in SPM-mm: <computeroutput>RSCT</computeroutput> to <computeroutput>RSCN</computeroutput>. This transformation is identity in fact because the template and normalized images are in the same referential internally in SPM, but it's maybe clearer if I mention it.
              </listitem>
            </itemizedlist>
          </listitem>
        </itemizedlist>
        <figure><title>Referentials and normalization transformations</title>
        <mediaobject>
          <imageobject role="fop"><imagedata fileref="&DIR_IMG;normalization.png" format="PNG"  width="150"/>
          </imageobject>
          <imageobject role="html"><imagedata fileref="&DIR_IMG;normalization.png" format="PNG"/>
          </imageobject>
        </mediaobject>
        </figure>
      </para>
    </sect3>

    <sect3><title>Resolution:</title>
      <para>
        We want first <computeroutput>A2T</computeroutput>, then <computeroutput>AIMS</computeroutput>, provided <computeroutput>Affine</computeroutput>, <computeroutput>S2MA</computeroutput> and <computeroutput>MT</computeroutput>
      </para>
      <para>
<screen>  <emphasis role="bold">A2T = MT * ( VsA * S2MA * At * Affine )^-1</emphasis>
  <emphasis role="bold">AIMS = TN^-1 * A2T</emphasis>
</screen>
      </para>
    </sect3>
  </sect2>
</sect1>


<sect1 id='aims_training%aimsinverttransformation' infoid='compute the inverse transformation'>
<title>Compute the inverse transformation</title>

<para><emphasis role='bold'>HELP:  </emphasis><ulink url='&PATH_WEB;#aims_AimsInvertTransformation' target='blank'>command help for AimsInvertTransformation</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<screen>prompt% AimsInvertTransformation -i R1_TO_R2.trm -o R2_TO_R1.trm</screen>
</sect1> 

<sect1 id='aims_training%aimscomposetransformation' infoid='Compose a transformation'>
<title>Compose a transformation</title> 

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsComposeTransformation' target='blank'>command help for AimsComposeTransformation</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>

<para>Let's imagine you have 3 referentials: R1, R2 and R3. You know R1-&gt;R2 (R1_TO_R2.trm) and R2-&gt;R3 (R2_TO_R3.trm). To compute R1-&gt;R3:</para>	
<screen>prompt% AimsComposeTransformation -i R2_TO_R3.trm R1_TO_R2.trm -o R1_TO_R3.trm</screen>
<para>NOTE_1: be aware, the order of transformation matrices is very important, this one is right <emphasis>-i R2_TO_R3.trm R1_TO_R2.trm</emphasis> but the following is completely wrong <emphasis>-i R1_TO_R2.trm R1_TO_R3.trm</emphasis>.</para>
<para>NOTE_2: if you have R3_TO_R2.trm and not R2_TO_R3.trm, you must first inverse this transformation matrix by using <link linkend='aims_training%aimsinverttransformation'>AimsInvertTransformation</link>	
	.</para>
</sect1> 

</chapter> 



<!--> <-->

<chapter>
<title>Handling graphs</title>
<sect1><title>Copy a set of graph attributes to another graph <!-- with the same structure--> </title>
	
<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsGraphConvert' target='blank'>command help for AimsGraphConvert</ulink></para>
		
<para><emphasis role='bold'>DATA: </emphasis>no data.</para>
	
<para>This case generally happens when working on automatically labelled sulci graphs. 
The nodes labels are given as the <emphasis>label</emphasis> attribute (automatic recognition labels), and you sometimes need to copy them to the <emphasis>name</emphasis> attribute (manual labelling). You have 2 possibilities to do it: manually or automatically.</para>
<para><emphasis role="bold">MANUALLY:</emphasis> you can verify each value of <emphasis>label</emphasis> attribute and correct it if necessary. To do so, change the value of the <emphasis>name</emphasis> attribute in a browser window (using a right-click on a graph node), and save the graph as a new graph (right-click on the graph in Anatomist control window and select <emphasis>File</emphasis> =&gt; <emphasis>Save</emphasis>).</para>

<para><emphasis role="bold">AUTOMATICALLY:</emphasis> you can use the <emphasis>AimsGraphConvert</emphasis> commandline. The following example shows how to use it:
							
<screen>prompt% AimsGraphConvert -i subjectAuto.arg -o subjectAutoName.arg -c label -d name </screen>
This command has many other options, but for the current application, the useful ones are:</para>

<itemizedlist>
<listitem><para><emphasis>-i option</emphasis>: input graph, for instance an autolabelled.</para></listitem>
<listitem><para><emphasis>-o option</emphasis>: output file name.<!-- For instance, you can name it by adding name of modified attribute. The associate .data directory is created by default with this name.--></para></listitem>
<listitem><para><emphasis>-c option</emphasis>: <!-- name of attribute uses like template (or for the copy)--> attribute to be copied.</para></listitem>
<listitem><para><emphasis>-d option</emphasis>: <!-- name of attribute for the destination--> destination attribute.</para></listitem>
<!-- use -b option to economize memory on computer -->
</itemizedlist>

</sect1>
</chapter>


<!-->  <-->

<chapter>
<title>Rigid registration</title>

<sect1 id='aims_training%aimsmanualregistration' infoid='using of AimsManualRegistration'>
<title>AimsManualRegistration: manual registration between 2 volumes from 3 specific landmarks.</title>

<para><emphasis role='bold'>KEYWORDS: </emphasis>.trm file.</para>	

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsManualRegistration' target='blank'>command help for AimsManualRegistration</ulink></para>

<para><emphasis role='bold'>DATA: </emphasis>no data.</para>	

<para><emphasis role='bold'>EXAMPLE: MANUAL REGISTRATION BETWEEN 2 VOLUMES FROM 3 SPECIFIC LANDMARKS.</emphasis></para> 
<para>This example is based on 2 volumes (i.e. registration from image_1 to image2) using specific points (i.e. anatomical landmarks). By using the AimsManualRegistration command line, you will obtain a transformation matrix from image_1 to image_2 as a .trm file.</para>
<itemizedlist>
<listitem><para>For each volume, <ulink url='#ana_training%draw_roi'>draw a ROI</ulink> with exactly 3 regions (each region must contain only 1 voxel) with the same name per region (ie voxel_1, voxel_2, and voxel_3). So, you have the following ROIs: <filename>ROI_image_1.arg</filename> and <filename>ROI_image_2.arg</filename> with the structure of each ROI is composed by the regions: voxel_1, voxel_2 and voxel_3.</para></listitem>
<listitem><para>The command line is as follows: </para>
<screen>prompt% AimsManualRegistration -f ROI_image_1.arg -t ROI_image_2.arg -o ROI_image_1_TO_ROI_image_2.trm
</screen>
</listitem>
</itemizedlist>   
<para>NOTE: <ulink url='#ana_training%load_transformation'>to load a transformation, please refer to <emphasis>The Anatomist getting started guide</emphasis></ulink>.</para> 
	
</sect1> 

<sect1 id='aims_training%aimsmiregister' infoid='using of AimsMIRegister'>
<title>AimsMIRegister: registration based on mutual information.</title>

<para><emphasis role='bold'>KEYWORDS: </emphasis>.trm file.</para>	

<para><emphasis role='bold'>HELP: </emphasis><ulink url='&PATH_WEB;#aims_AimsMIRegister' target='blank'>command help for AimsMIRegister</ulink></para>

<para><emphasis role='bold'>DATA:</emphasis>
<itemizedlist>
<listitem><filename>&DIR_DEMO;Registration/anat.img</filename> </listitem>
<listitem><filename>&DIR_DEMO;Registration/fonc.ima</filename></listitem>
</itemizedlist></para>

<para>This command can appeare complex because a lot of options are available. In this section, we are going to try to define a reasonable use. The easier use is the following:</para>
<screen>prompt% AimsMIRegister -r anat.img -t fonc.ima --dir fonc_TO_anat.trm --inv anat_TO_fonc.trm
</screen>
<?lb?>

<para>Here are some options to optimize the registration. It is not advisable to use the other because the implementation is not totally finished.
<?lb?><?lb?><emphasis role='bold'>Initialization of registration: --gcinit, --seuilref and --seuiltest</emphasis>. There are 2 modes for initialization of registration. The <emphasis role='bold'> --gcinit 1</emphasis> mode (default mode) allows an initialization with the center of gravity. It works with <emphasis role='bold'> --seuilref</emphasis> (0.05 by default) and <emphasis role='bold'> --seuiltest</emphasis> (0.1 by default) options. These thresholds preserve a percentage of intensity according to the maximun intensity. Historically, this command was created to register PET and T1 RMI modalities, so to decrease the signal of PET data, a threshold was performed. And the <emphasis role='bold'> --gcinit 0</emphasis> mode allows an initialization with coordinates by using of <emphasis role='bold'> --Tx, --Ty, --Tz, --dTx, --dTy, --dTz, --Rx, --Ry, --Rz, --dRx, --dRy, --dRz</emphasis>. Parameters beginning with a <emphasis role='bold'>d</emphasis> corresponds to the exploration step, which is voxel=/2 by default.</para> 

<para><emphasis role='bold'>Speeding up the process</emphasis>, reference image can be damaged with a reduction factor according to the principle of a pyramid with <emphasis role='bold'> --refstartpyr</emphasis>:</para>	
<itemizedlist>
	<listitem><para><emphasis role='bold'> --refstartpyr 1</emphasis>:  1 voxel for 2 in the 3 directions = reduction of factor 8</para></listitem>
	<listitem><para><emphasis role='bold'> --refstartpyr 2</emphasis>:  1 voxel for 4 in the 3 directions = reduction of factor 64</para></listitem>
	<listitem><para><emphasis role='bold'> --refstartpyr 3</emphasis>:  1 voxel for 8 in the 3 directions = reduction of factor 562</para></listitem>
</itemizedlist>


</sect1>


</chapter>


<!--> <-->

<chapter>
<title>Advanced level</title>
<sect1 id='aims_training%symmetrical_roi' infoid='Get a symmetrical ROI'>
<title>Get a symmetrical ROI</title> 


<para><emphasis role='bold'>HELP: </emphasis><itemizedlist>
<listitem><ulink url='&PATH_WEB;#aims_AimsMidPlaneAlign' target='blank'>command help for AimsMidPlaneAlign</ulink></listitem>
<listitem><ulink url='&PATH_WEB;#aims_AimsLinearComb' target='blank'>command help for AimsLinearComb</ulink></listitem>
<listitem><ulink url='&PATH_WEB;#aims_AimsResample' target='blank'>command help for AimsResample</ulink></listitem>
<listitem><ulink url='&PATH_WEB;#aims_AimsThreshold' target='blank'>command help for AimsThreshold</ulink></listitem>
<listitem><ulink url='&PATH_WEB;#aims_AimsFlip' target='blank'>command help for AimsFlip</ulink></listitem>
</itemizedlist></para>

<!-- REF
     IEEE Trans Med Imaging. 2002 Feb;21(2):122-38.
     Computation of the mid-sagittal plane in 3-D brain images.
     
     * Prima S,
     * Ourselin S,
     * Ayache N.
     
     Epidaure Project, INRIA, Sophia Antipolis, France. prima@bic.mni.mcgill.ca
     
     We present a new method to automatically compute, reorient, and recenter the mid-sagittal plane in anatomical and functional three-dimensional (3-D) brain images. This iterative approach is composed of two steps. At first, given an initial guess of the mid-sagittal plane (generally, the central plane of the image grid), the computation of local similarity measures between the two sides of the head allows to identify homologous anatomical structures or functional areas, by way of a block matching procedure. The output is a set of point-to-point correspondences: the centers of homologous blocks. Subsequently, we define the mid-sagittal plane as the one best superposing the points on one side and their counterparts on the other side by reflective symmetry. Practically, the computation of the parameters characterizing the plane is performed by a least trimmed squares estimation. Then, the estimated plane is aligned with the center of the image grid, and the whole process is iterated until convergence. The robust estimation technique we use allows normal or abnormal asymmetrical structures or areas to be treated as outliers, and the plane to be mainly computed from the underlying gross symmetry of the brain. The algorithm is fast and accurate, even for strongly tilted heads, and even in presence of high acquisition noise and bias field, as shown on a large set of synthetic data. The algorithm has also been visually evaluated on a large set of real magnetic resonance (MR) images. We present a few results on isotropic as well as anisotropic anatomical (MR and computed tomography) and functional (single photon emission computed tomography and positron emission tomography) real images, for normal and pathological subjects.
     
     PMID: 11929100 [PubMed - indexed for MEDLINE]

     
     Utilisaiton du mode pyramide : il part d'une estimation grossiere depuis un niveau 3 puis reinitialise pour travailler en niveau 2 et ainsi de suite. Le niveau de pyramide correspond en fait au nombre de voxel aggloméré.
     On pourrait faire : 
     start 3
     stop 2
    
     Note : il faut travailler sur des images en SHORT, sinon il faut faire une conversion.
     
     Note : Si on a R1, R2 et R3, ainsi que R1-&lt;R2, R2-&lt;R1 déduit de la precedente et R1-&lt;R3, 
     pour passer de R2 à R3, attention à l'odre des transfo dans AimsComposeTransformation - i R1-&lt;R3 R2-&lt;R1 !

     Note : Le seuillage permet de recuperer une ROI +/- corrig�e (d'env 1/2 voxel) du reechantillonnage.
     -->
	
<para><emphasis role='bold'>DATA: </emphasis>no data.</para>	
	
<para>The purpose of this section is for instance to compare the ROI measurement for both hemispheres following the realignment of the brain by using a symmetric axe<!--. This section is difficult enough because there is a lot of command lines-->:
<itemizedlist>	
	
<listitem><para> <ulink url='#ana_training%draw_roi'>Draw a ROI</ulink> on the T1 MRI and export it as a mask to work with a .ima file (image) and not a .arg (graph): <filename>roi.ima</filename><!-- Note: convert from .arg to .ima --> </para></listitem>

<listitem><para>Use the AimsMidPlaneAlign command line to realign the image and compute the transformation (superposition of the interhemispheric plane with the plane x=dimX/2).</para>
<screen>prompt% AimsMidPlaneAlign -i rmiT1.ima -o align_rmiT1.ima</screen>
<para>NOTE: the transformation matrix is located in the input file directory with the following name <filename>rmiT1.ima_TO_align.ima.trm</filename>.<!-- Sorry, this name file is not really right because the file extension should be <emphasis>.trm</emphasis> to indicate the type of file. If necessary, you can change it to be recognized automatically by an application such as Anatomist.--></para></listitem>

<listitem><para>Do a linear combination if the ROI is a binary image :<!--, then you must do the following command to extend the grey levels:--></para>
<screen>prompt% AimsLinearComb -i roi.ima -o linearComb_roi.ima -a 16000</screen>
<para>NOTE: please refer to the table in NOTE_2 if the image is not binary.</para>
</listitem>

<listitem><para>Resample the ROI with the previously calculated transformation:</para>
<screen>prompt% AimsResample -i linearComb_roi.ima -o resample_roi.ima -m rmiT1.ima_TO_align.ima.trm</screen></listitem>

<listitem><para>Perform a threshold to 8000 to preserve a correct volume because the resampling widely extend the symmetric roi volume:</para>
<screen>prompt% AimsThreshold -i resample_roi.ima -o threshold_roi.ima -m ge -t 8000</screen></listitem>

<listitem><para>Get the symmetrical ROI by using AimsFlip as follows:</para>
<screen>prompt% AimsFlip -i threshold_roi.ima -o sym_roi.ima -m XX </screen></listitem>

<listitem><para>Each ROI can be in both referentials which are T1 and T1_align. In order to change the coordinate system, you apply a .trm. For instance, if you want the ROIs in T1 referential, you must resample the sym_roi.ima with the inverse of T1MRI.ima_TO_align.ima.</para>
<screen>prompt% AimsInvertTransformation -i rmiT1.ima_TO_align.ima.trm -o align.ima_TO_rmiT1.ima.trm</screen>
<para>Then, resample the sym_roi.ima: </para>
<screen>prompt% AimsResample -i sym_roi.ima -o sym_roi_RT1.ima -m align.ima_TO_rmiT1.ima.trm</screen>
</listitem>

<listitem><para>Analyze/compare the ROIs by using <link linkend='aims_training%aimsroifeatures'>AimsRoiFeatures</link>.</para></listitem>

</itemizedlist>	


</para> 

<para>NOTE_1: be aware that  the procedure presented below is not formal. In fact, many variations can be processed, the modality (PET, CT ...), how the ROI is obtained (draw on the original referential, or after the realignment) or where it comes from (i.e. created by an other process), what is the type of ROI value (binary, label image ...).<!-- In other words, each case can be different. Don't hesitate to test many ways.--></para>

<para>NOTE_2: here is a summary to help you compute and/or do a threshold of your ROI to preserve a correct volume (the resampling leads to volume changes):  
<table>
<title>Summary to preserve the ROI volume</title>
<tgroup cols="2">
<thead>
<row>
<entry>Max value</entry>
<entry>commande line</entry>
</row>
</thead>
<tbody>
<row>
<entry><para>Binary image (max=1)</para></entry>
<entry>
<para>
<screen>prompt% AimsLinearComb -i roi.ima -o roi.ima -a 16000
prompt% AimsResample -i roi.ima -o roi.ima -m motion.trm
prompt% AimsThreshold -i roi.ima -o roi.ima -m ge -t 8000
</screen>
</para>
</entry>
</row>
<row>
<entry><para>Max=max_value</para></entry>
<entry><para><screen>prompt% AimsLinearComb -i roi.ima -o roi.ima -a 16000 -b max_value
prompt% AimsResample -i roi.ima -o roi.ima -m motion.trm
prompt% AimsThreshold -i roi.ima -o roi.ima -m ge -t 8000
</screen></para></entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>NOTE_3: for more information on the algorithm used by AimsMidPlaneAlign, please refer to <emphasis>Prima S, Ourselin S, and Ayache N. Computation of the mid-sagittal plane in 3-D brain images. IEEE Trans Med Imaging. 2002 Feb;21(2):122-38</emphasis>.</para>

</sect1>

</chapter> 






<!--

<chapter>
<title>Advanced level</title>

<para></para>
</chapter>


<chapter>
<title>Expert level</title>

<para></para>
</chapter>
 -->



<!--> <-->

<chapter id='aims_training%pyaims' infoid='Programming with AIMS in Python language'>
  <title>Programming with AIMS in Python language</title>

  <para>AIMS is a C++ library, but has python language bindings: <emphasis role="bold">PyAIMS</emphasis>. This means that the C++ classes and functions can be used from python. This has many advantages compared to pure C++:
    <itemizedlist>
      <listitem>Writing python scripts and programs is much easier and faster than C++: there is no fastidious and long compilation step.
      </listitem>
      <listitem>Scripts are more flexible, can be modified on-the-fly, etc
      </listitem>
      <listitem>It can be used interactively in a python interactive shell.
      </listitem>
      <listitem>As pyaims is actually C++ code called from python, it is still fast to execute complex algorithms. There is obviously an overhead to call C++ from python, but once in the C++ layer, it is C++ execution speed.
      </listitem>
    </itemizedlist>
    A few examples of how to use and manipulate the main data structures will be shown here.
  </para>
  <para>
    The data for the examples in this section is the data of the JIRFNI 2008 training course, and can be downloaded here: <ulink url="ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data.zip">ftp://ftp.cea.fr/pub/dsv/anatomist/data/demo_data.zip</ulink>. To use the examples directly, users should go to the directory where this archive was uncompressed, and then run ipthon from this directory. A cleaner alternative, especially if no write access is allowed on this data directory, is to make a symbolic link to the <computeroutput>"data_for_anatomist"</computeroutput> subdirectory:
    <screen>cd /scratch/user
mkdir bvcourse
cd bvcourse
ln -s &lt;path_ti_data&gt;/data_for_anatomist .</screen>
  </para>

  <sect1>
    <title>Using data structures</title>

    <sect2>
      <title>Module importation</title>
      <para>In python, the aimsdata library is available as the <computeroutput>soma.aims</computeroutput> module.
        <screen>import soma.aims
# the module is actually soma.aims:
vol = soma.aims.Volume_S16( 100, 100, 100 )</screen>
        or:
        <screen>from soma import aims
# the module is available as aims (not soma.aims):
vol = aims.Volume_S16( 100, 100, 100 )
# in the following, we will be using this form because it is shorter.</screen>
      </para>
    </sect2>

    <sect2>
      <title>IO: reading and writing objects</title>

      <para>
        Reading operations are accessed via a single <computeroutput>read()</computeroutput> function, and writing through a single <computeroutput>write()</computeroutput> function. The <computeroutput>read()</computeroutput> function reads any object from a given file name, in any supported file format, and returns it:
        <screen>from soma import aims
obj = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
print obj
obj2 = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
print obj2
obj3 = aims.read( 'data_for_anatomist/subject01/subject01_Lhemi.mesh' )
print obj3</screen>
        The returned object can have various types according to what is found in the disk file(s).
      </para>
      <para>
        Writing is just as easy. The file name extension generally determines the output format. An object read from a given format can be re-written in any other supported format, provided the format can actually store the object type.
        <screen>from soma import aims
obj2 = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
aims.write( obj2, 'Audio-Video_T_map.ima' )
obj3 = aims.read( 'data_for_anatomist/subject01/subject01_Lhemi.mesh' )
aims.write( obj3, 'subject01_Lhemi.gii' )</screen>
      </para>
      <highlights><emphasis role="bold">Exercise:</emphasis> write a little file format conversion tool
      </highlights>
    </sect2>

    <sect2>
      <title>Volumes</title>

      <para>
        Volumes are array-like containers of voxels, plus a set of additional information kept in a header structure. In AIMS, the header structure is generic and extensible, and does not depend on a specific file format. Voxels may have various types, so a specific type of volume should be used for a specific type of voxel. The type of voxel has a code that is used to suffix the Volume type: <computeroutput>Volume_S16</computeroutput> for signed 16-bit ints, <computeroutput>Volume_U32</computeroutput> for unsigned 32-bit ints, <computeroutput>Volume_DOUBLE</computeroutput> for 32-bit floats, <computeroutput>Volume_RGBA</computeroutput> for RGBA colors, etc.
      </para>

      <sect3><title>Building a volume</title>
        <para>
          <screen># create a 3D volume of signed 16-bit ints, of size 192x256x128
vol = aims.Volume_S16( 192, 256, 128 )
# fill it with zeros
vol.fill(0)
# set value 12 at voxel ( 100, 100, 60 )
vol.setValue( 12, 100, 100, 60 )
# get value at the same position
x = vol.value( 100, 100, 60 )
# set the voxels size
vol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
print vol.header()</screen>
        </para>
      </sect3>

      <sect3><title>Basic operations:</title>
        <para>
          Whole volume operations:
          <screen># multiplication, addition etc
vol *= 2
vol2 = vol * 3 + 12
vol /= 2
vol3 = vol2 - vol - 12
vol4 = vol2 * vol / 6</screen>
          Voxel-wise operations:
          <screen># fill the volume with the distance to voxel ( 100, 100, 60 )
vs = vol.header()[ 'voxel_size' ]
pos0 = ( 100 * vs[0], 100 * vs[1], 60 * vs[2] ) # in millimeters
for z in xrange( vol.getSizeZ() ):
  for y in xrange( vol.getSizeY() ):
    for x in xrange( vol.getSizeX() ):
      # get current position in an aims.Point3df structure, in mm
      p = aims.Point3df( x * vs[0], y * vs[1], z * vs[2] )
      # get relative position to pos0, in voxels
      p -= pos0
      # distance: norm of vector p
      dist = p.norm()
      # set it into the volume
      vol.setValue( dist, x, y, z )
# save the volume
aims.write( vol, 'distance.nii' )</screen>
        Now look at the <computeroutput>"distance.nii"</computeroutput> volume in Anatomist.
        </para>
        <para>
          <highlights><emphasis role="bold">Exercise:</emphasis> Make a program which loads the image <computeroutput>data_for_anatomist/subject01/Audio-Video_T_map.nii</computeroutput> and thresholds it so as to keep values above 3.
          </highlights>
          <screen>from soma import aims
vol = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
for z in xrange( vol.getSizeZ() ):
  for y in xrange( vol.getSizeY() ):
    for x in xrange( vol.getSizeX() ):
      if vol.value( x, y, z ) &lt; 3.:
        vol.setValue( 0, x, y, z )
aims.write( vol, 'Audio-Video_T_thresholded.nii' )</screen>
        </para>
        <para>
          <highlights><emphasis role="bold">Exercise:</emphasis> Make a program to dowsample the anatomical image <computeroutput>data_for_anatomist/subject01/subject01.nii</computeroutput> and keeps one voxel out of two in every direction.
          </highlights>
          <screen>from soma import aims
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
# allocate a new volume with half dimensions
vol2 = aims.Volume_DOUBLE( vol.getSizeX() / 2, vol.getSizeY() / 2, vol.getSizeZ() / 2 )
# set the voxel size to twice it was in vol
vs = vol.header()[ 'voxel_size' ]
vs2 = [ x * 2 for x in vs ]
vol2.header()[ 'voxel_size' ] = vs2
for z in xrange( vol2.getSizeZ() ):
  for y in xrange( vol2.getSizeY() ):
    for x in xrange( vol2.getSizeX() ):
      vol2.setValue( vol.value( x*2, y*2, z*2 ), x, y, z )
aims.write( vol2, 'resampled.nii' )</screen>
        </para>
        <para>
          The first thing that comes to mind when running these examples, is that they are <emphasis>slow</emphasis>. Indeed, python is an interpreted language and loops in any interpreted language are slow. In addition, accessing individually each voxel of the volume has the overhead of python/C++ bindings communications. The conclusion is that that kind of example is probably a bit too low-level, and should be done, when possible, by compiled libraries or specialized array-handling libraries. This is the role of <emphasis role="bold">numpy</emphasis>.
        </para>
        <para>
          Accessing numpy arrays to AIMS volume voxels is supported:
          <screen>import numpy
vol.fill( 0 )
arr = numpy.array( vol, copy=False )
# set value 100 in a whole sub-volume
arr[60:120, 60:120, 40:80] = 100
# note that arr is a shared view to the volume contents,
# modifications will also affect the volume
aims.write( vol, "cube.nii" )</screen>
        </para>
        <para>
          Now we can re-write thethresholding example using numpy:
          <screen>from soma import aims
vol = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
arr = numpy.array( vol, copy=False )
arr[ numpy.where( arr &lt; 3. ) ] = 0.
aims.write( vol, 'Audio-Video_T_thresholded2.nii' )</screen>
          Here, <computeroutput>arr &lt; 3.</computeroutput> returns a boolean array with the same size as <computeroutput>arr</computeroutput>, and <computeroutput>numpy.where()</computeroutput> returns arrays of coordinates where the specified contition is true.
        </para>
        <para>
          The distance example, using numpy, would like the following:
          <screen>from soma import aims
import numpy
vol = aims.Volume_S16( 192, 256, 128 )
vol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
vs = vol.header()[ 'voxel_size' ]
pos0 = ( 100 * vs[0], 100 * vs[1], 60 * vs[2] ) # in millimeters
arr = numpy.array( vol, copy=False )
# build arrays of coordinates for x, y, z
x, y, z = numpy.ogrid[ 0.:vol.getSizeX(), 0.:vol.getSizeY(), 0.:vol.getSizeZ() ]
# get coords in millimeters
x *= vs[0]
y *= vs[1]
z *= vs[2]
# relative to pos0
x -= pos0[0]
y -= pos0[1]
z -= pos0[2]
# get norm, using numpy arrays broadcasting
arr[:,:,:,0] = numpy.sqrt( x**2+y**2+z**2 )
# and save result
aims.write( vol, 'distance2.nii' )</screen>
        This example appears a bit more tricky, since we must build the coordinates arrays, but is way faster to execute, because all loops within the code are executed in compiled routines in numpy. One interesting thing to note is that this code is using the famous "array broadcasting" feature of numpy, where arrays of heterogeneous sizes can be combined, and the "missing" dimensions are extended.
        </para>
      </sect3>
      <sect3>
        <title>Copying volumes or volumes structure, or building from an array</title>
        <para>
          To make a deep-copy of a volume, use the copy constructor:
          <screen>vol2 = aims.Volume_S16( vol )
vol2.setValue( 12, 100, 100, 60 )
# now vol and vol2 have different values
print 'vol.value( 100, 100, 60 ):', vol.value( 100, 100, 60 )
print 'vol2.value( 100, 100, 60 ):', vol2.value( 100, 100, 60 )</screen>
          If you need to build another, different volume, with the same structure and size, don't forget to copy the header part:
          <screen>vol2 = aims.Volume_FLOAT( vol.getSizeX(), vol.getSizeY(), vol.getSizeZ(), vol.getSizeT() )
vol2.header().update( vol.header() )</screen>
          Important information can reside in the header, like voxel size, or coordinates systems and geometric transformations to other coordinates systems, so it is really very important to carry this information with duplicated or derived volumes.
        </para>
        <para>
          You can also build a volume from a numy array:
          <screen>arr = numpy.array( numpy.diag( xrange( 40 ) ), dtype=numpy.float32 ).reshape( 40, 40, 1 ) \
    + numpy.array( xrange( 20 ), dtype=numpy.float32 ).reshape( 1, 1, 20 )
# WARNING: the array must be in Fortran ordering for AIMS, at leat at the moment
# whereas the numpy addition always returns a C-ordered array
arr = numpy.array( arr, order='F' )
arr[ 10, 12, 3 ] = 25
vol = aims.Volume_FLOAT( arr )
print 'vol.value( 10, 12, 3 ):', vol.value( 10, 12, 3 )
# data are shared with arr
vol.setValue( 35, 10, 15, 2 )
print 'arr[ 10, 15, 2 ]:', arr[ 10, 15, 2 ]
arr[ 12, 15, 1 ] = 44
print 'vol.value( 12, 15, 1 ):', vol.value( 12, 15, 1 )</screen>
        </para>
      </sect3>
      <sect3><title>4D volumes</title>
        <para>
          4D volumes work just like 3D volumes. Actually all volumes are 4D in AIMS, but the last dimension is commonly of size 1. In <computeroutput>value()</computeroutput> and <computeroutput>setValue()</computeroutput> methods, only the first dimension is mandatory, others are optional and default to 0, but up to 4 coordinates may be used. In the same way, the constructor takes up to 4 dimension parameters:
          <screen>from soma import aims
# create a 4D volume of signed 16-bit ints, of size 30x30x30x4
vol = aims.Volume_S16( 30, 30, 30, 4 )
# fill it with zeros
vol.fill(0)
# set value 12 at voxel ( 10, 10, 20, 2 )
vol.setValue( 12, 10, 10, 20, 2 )
# get value at the same position
x = vol.value( 10, 10, 20, 2 )
# set the voxels size
vol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
print vol.header()</screen>
          Similarly, 1D or 2D volumes may be used exactly the same way.
        </para>
      </sect3>
      <sect3><title>The older AimsData classes</title>
        <para>
          For historical reasons, another set of classes may also represent volumes. These classes are the older API in AIMS, and tend to be obsolete. But as they were used in many many routines and programs, they have still not been eradicated. Many C++ routines build volumes and actually return those older classes, so we could not really hide them, and they also have python bindings. These classes are <computeroutput>aims.AimsData_&lt;type&gt;</computeroutput>. Converting from and to <computeroutput>aims.Volume_</computeroutput> classes is rather simple since the newer <computeroutput>Volume</computeroutput> classes are used internally in the <computeroutput>AimsData</computeroutput> API.
          <screen>from soma import aims
# create a 4D volume of signed 16-bit ints, of size 30x30x30x4
vol = aims.Volume_S16( 30, 30, 30, 4 )
vol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
advol = aims.AimsData_S16( vol )
# vol and advol share the same header and voxel data
vol.setValue( 12, 10, 10, 20, 2 )
print 'advol.value( 10, 10, 20, 2 ):', advol.value( 10, 10, 20, 2 )
advol.setValue( 44, 12, 12, 24, 1 )
print 'vol.value( 12, 12, 24, 1 ):', vol.value( 12, 12, 24, 1 )</screen>
          And, in the other side:
          <screen>from soma import aims
# create a 4D volume of signed 16-bit ints, of size 30x30x30x4
advol = aims.AimsData_S16( 30, 30, 30, 4 )
advol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
vol = advol.volume()
# vol and advol share the same header and voxel data
vol.setValue( 12, 10, 10, 20, 2 )
print 'advol.value( 10, 10, 20, 2 ):', advol.value( 10, 10, 20, 2 )
advol.setValue( 44, 12, 12, 24, 1 )
print 'vol.value( 12, 12, 24, 1 ):', vol.value( 12, 12, 24, 1 )</screen>
          <computeroutput>AimsData</computeroutput> has a bit richer API, since it includes minor processing functions that have been removed from the newer <computeroutput>Volume</computeroutput> for the sake of API simplicity and minimalism.
          <screen># minimum / maximum
print 'min:', advol.minimum(), 'at', advol.minIndex()
print 'max:', advol.maximum(), 'at', advol.maxIndex()
# clone copy
advol2 = advol.clone()
advol2.setValue( 12, 4, 8, 11, 3 )
# now advol and advol2 have different values
print 'advol.value( 4, 8, 11, 3 ):', advol.value( 4, 8, 11, 3 )
print 'advol2.value( 4, 8, 11, 3 ):', advol2.value( 4, 8, 11, 3 )
# Border handling
# Border width is th 5th parameter of AimsData constructor
advol = aims.AimsData_S16( 192, 256, 128, 1, 2 )
advol.header()[ 'voxel_size' ] = [ 0.9, 0.9, 1.2, 1. ]
advol.fill( 0 )
advol.setValue( 15, 100, 100, 60 )
vol = advol.volume()
# then vol is 4 voxels wider in each direction, and shifted:
print 'vol.value( 100, 100, 60 ):', vol.value( 100, 100, 60 )
# ... it is 0, not 15...
print 'vol.value( 102, 102, 62 ):', vol.value( 102, 102, 62 )
# here we get 15
# some algorithms require this border to exist, otherwise fail or crash...
from soma import aimsalgo
aimsalgo.AimsDistanceFrontPropagation( advol, 0, -1, 3, 3, 3, 10, 10 )
aims.write( advol, 'distance3.nii' )</screen>
        </para>
      </sect3>
    </sect2>

    <sect2>
      <title>Meshes</title>

      <sect3><title>Structure</title>
        <para>
        A surfacic mesh represents a surface, as a set of small polygons (genrally triangles, but sometimes quads). It has two main components: a vector of vertices (each vertex is a 3D point, with coordinates in millimeters), and a vector of polygons: each polygon is defined by the vertices it links (3 for a triangle). It also optionally has normals (unit vectors). In our mesh structures, there is one normal for each vertex.
        <screen>from soma import aims
mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lhemi.mesh' )
vert = mesh.vertex()
print 'vertices:', len( vert )
poly = mesh.polygon()
print 'polygons:', len( poly )
norm = mesh.normal()
print 'normals:', len( norm )</screen>
        To build a mesh, we can instantiate an object of type <computeroutput>aims.AimsTimeSurface_&lt;n&gt;</computeroutput>, with <emphasis>n</emphasis> being the number of vertices by polygon. Then we can add vertices, normals and polygons to the mesh:
        <screen># build a flying saucer mesh
from soma import aims
import numpy
mesh = aims.AimsTimeSurface_3()
# a mesh has a header
mesh.header()[ 'toto' ] = 'a message in the header'
vert = mesh.vertex()
poly = mesh.polygon()
x = numpy.cos( numpy.ogrid[ 0.:20 ] * numpy.pi / 10. ) * 100
y = numpy.sin( numpy.ogrid[ 0.:20 ] * numpy.pi / 10. ) * 100
z = numpy.zeros( 20 )
c = numpy.vstack( ( x, y, z ) ).transpose()
vert.assign( [ aims.Point3df( 0., 0., -40. ), aims.Point3df( 0., 0., 40. ) ] + [ aims.Point3df( x ) for x in c ] )
pol = numpy.vstack( ( numpy.zeros( 20, dtype=numpy.int32 ), numpy.ogrid[ 3:23 ], numpy.ogrid[ 2:22 ] ) ).transpose()
pol[ 19, 1 ] = 2
pol2 = numpy.vstack( ( numpy.ogrid[ 2:22 ], numpy.ogrid[ 3:23 ], numpy.ones( 20, dtype=numpy.int32 ) ) ).transpose()
pol2[19,1] = 2
poly.assign( [ aims.AimsVector_U32_3(x) for x in numpy.vstack( ( pol, pol2 ) ) ] )
# write result
aims.write( mesh, 'saucer.mesh' )
# automatically calculate normals
mesh.updateNormals()</screen>
        </para>
      </sect3>

      <sect3><title>Modifying a mesh</title>
        <para>
          <screen># slightly inflate a mesh
from soma import aims
import numpy
mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
vert = mesh.vertex()
varr = numpy.array( vert )
norm = numpy.array( mesh.normal() )
varr += norm * 2 # push vertices 2mm away along normal
vert.assign( [ aims.Point3df(x) for x in varr ] )
mesh.updateNormals()
aims.write( mesh, 'subject01_Lwhite_semiinflated.mesh' )</screen>
        Now look at both meshes in Anatomist...<sbr/>
        Alternatively, without numpy, we could have written the code like this:
        <screen>from soma import aims
mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
vert = mesh.vertex()
norm = mesh.normal()
for v, n in zip( vert, norm ):
  v += n * 2
mesh.updateNormals()
aims.write( mesh, 'subject01_Lwhite_semiinflated.mesh' )</screen>
        </para>
      </sect3>

      <sect3><title>Handling time</title>
        <para>
          In AIMS, meshes are actually time-indexed dictionaries of meshes. This way a deforming mesh can be stored in the same object. To copy a timestep to aonother, use the following:
          <screen>from soma import aims
mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
# mesh.vertex() is equivalent to mesh.vectex( 0 )
mesh.vertex( 1 ).assign( mesh.vertex( 0 ) )
# same for normals and polygons
mesh.normal( 1 ).assign( mesh.normal( 0 ) )
mesh.polygon( 1 ).assign( mesh.polygon( 0 ) )
print 'number of time steps:', mesh.size()</screen>
          <highlights><emphasis role="bold">Exercise:</emphasis> make a deforming mesh that goes from the original mesh to 5mm away, by steps of 0.5 mm
          </highlights>
          <screen>from soma import aims
import numpy
mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
vert = mesh.vertex()
varr = numpy.array( vert )
norm = numpy.array( mesh.normal() )
for i in xrange( 1, 10 ):
  vert.assign( mesh.vertex() )
  mesh.normal( i ).assign( mesh.normal() )
  mesh.polygon( i ).assign( mesh.polygon() )
  varr += norm * 0.5
  mesh.vertex( i ).assign( [ aims.Point3df(x) for x in varr ] )
mesh.updateNormals()
# sorry, this is a bug in AIMS 3.2.0 IO system... it is fixed for 3.2.1
if mesh.header().has_key( 'nb_t_pos' ):
  del mesh.header()[ 'nb_t_pos' ]
aims.write( mesh, 'subject01_Lwhite_semiinflated_time.mesh' )</screen>
        </para>
      </sect3>
    </sect2>

    <sect2>
      <title>Textures</title>

      <para>A texture is merely a vector of values, each of them is assigned to a mesh vertex, with a one-to-one mapping, in the same order. A texture is also a time-texture.
        <screen>from soma import aims
tex = aims.TimeTexture_FLOAT()
t = tex[0] # time index, inserts on-the-fly
t.reserve( 10 ) # pre-allocates memory
for i in xrange( 10 ):
  t.append( i / 10. )</screen>
      </para>
      <para>
        <highlights><emphasis role="bold">Exercise:</emphasis> make a time-texture, with at each time/vertex of the previous mesh, sets the value of the underlying volume <computeroutput>data_for_anatomist/subject01/subject01.nii</computeroutput>
        </highlights>
        <screen>from soma import aims
mesh = aims.read( 'subject01_Lwhite_semiinflated_time.mesh' )
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
tex = aims.TimeTexture_FLOAT()
vs = vol.header()[ 'voxel_size' ]
for i in xrange( mesh.size() ):
  t = tex[i]
  vert = mesh.vertex( i )
  t.reserve( len( vert ) )
  for p in vert:
    t.append( vol.value( *[ int( round(x/y) ) for x,y in zip( p, vs ) ] ) )
aims.write( tex, 'subject01_Lwhite_semiinflated_texture.tex' )</screen>
        Now look at the texture on the mesh (inflated or not) in Anatomist. Compare it to a 3D fusion between the mesh and the MRI volume.
      </para>
      <para><emphasis role="bold">Bonus:</emphasis> We can do the same for functional data. But in this case we may have a spatial transformation to apply between anatomical data and functional data (which may have been normalized, or acquired in a different referential).
        <screen>from soma import aims
import numpy
mesh = aims.read( 'subject01_Lwhite_semiinflated_time.mesh' )
vol = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
# get header info from anatomical volume
f = aims.Finder()
f.check( 'data_for_anatomist/subject01/subject01.nii' )
anathdr = f.header()
# get functional -> MNI transformation
m1 = aims.Motion( vol.header()[ 'transformations' ][1] )
# get anat -> MNI transformation
m2 = aims.Motion( anathdr[ 'transformations' ][1] )
# make anat -> functional transformation
anat2func = m1.inverse() * m2
# include functional voxel size to get to voxel coordinates
vs = vol.header()[ 'voxel_size' ]
mvs = aims.Motion( numpy.diag( vs[:3] + [ 1. ] ) )
anat2func = mvs.inverse() * anat2func
# now go as in the previous program
tex = aims.TimeTexture_FLOAT()
for i in xrange( mesh.size() ):
  t = tex[i]
  vert = mesh.vertex( i )
  t.reserve( len( vert ) )
  for p in vert:
    t.append( vol.value( *[ int(round(x)) for x in anat2func.transform( p ) ] ) )
aims.write( tex, 'subject01_Lwhite_semiinflated_audio_video.tex' )</screen>
        See how the functional data on the mesh changes across the depth of the cortex. this demonstrates the need to have a proper projection of functional data before dealing with surfacic functional processing.
      </para>
    </sect2>

    <sect2>
      <title>Buckets</title>

      <para>
        "Buckets" are voxels lists. They are typically used to represent ROIs.
      </para>
    </sect2>

    <sect2>
      <title>Graphs</title>

      <para>
        Graphs are data structures that may contain various elements. They can represent sets of smaller structures, and also relations between such structures. The main usage we have for them is to represent ROIs sets, sulci, or fiber bundles.
      </para>
      <para>
        A graph contains:
        <itemizedlist>
          <listitem>properties of any type, like a volume or mesh header.</listitem>
          <listitem>nodes (also called vertices), which represent structured elements (a ROI, a sulcus part, etc), which in turn can store properties, and geometrical elements: buckets, meshes...
          </listitem>
          <listitem>optionally, relations, which link nodes and can also contain properties and geometrical elements.</listitem>
        </itemizedlist>
      </para>

      <sect3><title>Properties</title>
        <para>
          Properties are stored in a dictionary-like way. They can hold almost anything, but a restricted set of types can be saved and loaded. It is exactly the same thing as headers found in volumes, meshes, textures or buckets.
          <screen>from soma import aims
graph = aims.read( 'data_for_anatomist/roi/basal_ganglia.arg' )
print graph
print 'properties:', graph.keys()
for p, v in graph.iteritems():
  print p, ':', v
graph[ 'gudule' ] = [ 12, 'a comment' ]</screen>
          <emphasis rome="bold">Note:</emphasis> Only properties declared in a "syntax" file may be saved and re-loaded. Other properties are just not saved.
        </para>
      </sect3>

      <sect3><title>Vertices</title>
        <para>
          Vertices (or nodes) can be accessed via the vertices() method. Each vertex is also a dictionary-like properties set.
          <screen>for v in graph.vertices():
  print v['name']</screen>
        </para>
        <para>
          To insert a new vertex, the <computeroutput>addVertex()</computeroutput> method should be used:
          <screen>v = graph.addVertex( 'roi' )
print v
v[ 'name' ] = 'new ROI'</screen>
        </para>
      </sect3>

      <sect3><title>Edges</title>
        <para>
          An edge, or relation, links nodes together. Up to now we have always used binary, unoriented, edges. They can be added using the <computeroutput>addEdge()</computeroutput> method. Edges are also dictionary-like properties sets.
          <screen>v2 = graph.vertices().list()[1]
e = graph.addEdge( v.get(), v2, 'roi_link' )
print graph.edges()
# get vertices linked by this edge
print e.vertices()</screen>
        </para>
      </sect3>

      <sect3><title>Adding meshes or buckets in a graph vertex or relation</title>
        <para>
          Setting meshes or buckets in vertices properties is OK internally, but for saving and loading, additional consistancy must be ensured and internal tables update is required. Then, use the <computeroutput>aims.GraphManip.storeAims</computeroutput> function:
          <screen>mesh = aims.read( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
# store mesh in the 'roi' property of vertex v of graph graph
aims.GraphManip.storeAims( graph, v.get(), 'roi', mesh )
# the v.get() is a wrapper conversion that should not have been visible to
# users - we will try to eliminate this in future versions</screen>
        </para>
      </sect3>
    </sect2>

  </sect1>


  <sect1>
    <title>Using algorithms</title>

    <para>
      AIMS contains, in addition to the different data structures used in neuroimaging, a set of algorithms which operate on these structures. Currently only a few of them have Python bindings, because we develop these bindings in a "lazy" way, only when they are needed. The algorthms currently available include data conversion, resampling, thresholding, mathematical morphology, distance maps, the mesher, some mesh generators, and a few others. But most of the algorithms are still only available in C++.
    </para>

    <sect3><title>Volume Thresholding</title>
      <para>
        <screen>from soma import aims, aimsalgo
# read a volume with 2 voxels border
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii', border=2 )
# use a thresholder which will keep values above 600
ta = aims.AimsThreshold_S16_S16( aims.AIMS_GREATER_OR_EQUAL_TO, 600 )
# use it to make a binary thresholded volume
tvol = ta.bin( vol )
aims.write( tvol, 'thresholded.nii' )</screen>
      </para>
    </sect3>

    <sect3><title>Mathematical morphology</title>

      <para>
        <screen>
# apply 5mm closing
clvol = aimsalgo.AimsMorphoClosing( tvol, 5 )
aims.write( clvol, 'closed.nii' )</screen>
      </para>
    </sect3>

    <sect3><title>Mesher</title>
      <para>
        <screen>import numpy
m = aimsalgo.Mesher()
mesh = aims.AimsSurfaceTriangle() # create an empty mesh
# the border should be -1
clvol.fillBorder( -1 )
# get a smooth mesh of the interface of the biggest connected component
m.getBrain( clvol, mesh )
aims.write( mesh, 'head_mesh.gii' )</screen>
      </para>

      <para>The above examples make up a simplified version of the head mesh extraction algorithm in <computeroutput>VipGetHead</computeroutput>, used in the T1 pipeline.
      </para>
    </sect3>

    <sect3><title>Surface generation</title>
      <para>The <computeroutput>aims.SurfaceGenerator</computeroutput> allows to create simple meshes of predefined shapes: cube, cylinder, sphere, icosehedron, cone, arrow.
        <screen>from soma import aims
center = ( 50, 25, 20 )
radius = 53
mesh1 = aims.SurfaceGenerator.icosahedron( center, radius )
# this dictionary-based generation will work correctly in pyaims >= 3.2.1
mesh2 = aims.SurfaceGenerator.generate( { 'type' : 'arrow', 'point1' : [ 30, 70, 0 ],
  'point2' : [ 100, 100, 100 ], 'radius' : 20, 'arrow_radius' : 30,
  'arrow_length_factor' : 0.7, 'facets' : 50 } )
# get the list of all possible generated objects and parameters:
print aims.SurfaceGenerator.description()</screen>
      </para>
    </sect3>

    <sect3><title>Interpolation</title>
      <para>
        Interpolators help to get values in millimeters coordinates in a discrete space (volume grid), and may allow voxels values mixing (linear interpolation, typically).
        <screen>from soma import aims
# load a functional volume
vol = aims.read( 'data_for_anatomist/subject01/Audio-Video_T_map.nii' )
# get the position of the maximum
pmax, maxval = aims.AimsData_DOUBLE( vol ).maxIndex()
# set pmax in mm
vs = vol.header()[ 'voxel_size' ]
pmax = [ x * y for x,y in zip( pmax, vs ) ]
# take a sphere of 5mm radius, with about 200 vertices
mesh = aims.SurfaceGenerator.sphere( pmax[:3], 5., 200 )
vert = mesh.vertex()
# get an interpolator
interpolator = aims.aims.getLinearInterpolator( vol )
# create a texture for that sphere
tex = aims.TimeTexture_FLOAT()
tx = tex[0]
tx2 = tex[1]
tx.reserve( len( vert ) )
tx2.reserve( len( vert ) )
for v in vert:
  tx.append( interpolator.value( v ) )
  # compare to non-interpolated value
  tx2.append( vol.value( *[ int( round(x/y) ) for x,y in zip( v, vs ) ] ) )
aims.write( tex, 'functional_tex.gii' )
aims.write( mesh, 'sphere.gii' )</screen>
        Look at the difference between the two timesteps (interpolated and non-interpolated) of the texture in Anatomist.
      </para>
    </sect3>

    <sect3><title>Types conversion</title>
      <para>
        The <computeroutput>Converter_*_*</computeroutput> classes allow to convert some data structures types to others. Of course all types cannot be converted to any other, but they are typically used ton convert volumed from a given voxel type to another one. For instance, to convert the anatomical volume of the previous examples to float type:
        <screen>from soma import aims
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
print 'type of vol:', type( vol )
c = aims.Converter_Volume_S16_Volume_FLOAT()
vol2 = c( vol )
print 'type of converted volume:', type( vol2 )
print 'value of initial volume at voxel (50,50,50):', vol.value( 50, 50, 50 )
print 'value of converted volume at voxel (50,50,50):', vol2.value( 50, 50, 50 )</screen>
      </para>
    </sect3>

    <sect3><title>Resampling</title>
      <para>
        Resampling allows to apply a geometric transformation or/and to change voxels size. Several types of resampling may be used depending on how we interpolate values between neighbouring voxels (see interpolators): nearest-neighbour (order 0), linear (order 1), spline resampling with order 2 to 7 in AIMS.
        <screen>from soma import aims, aimsalgo
import math
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
# create an affine transformation matrix
# rotating pi/8 along z axis
tr = aims.Motion( aims.Quaternion( [ 0, 0, math.sin( math.pi/16 ), math.cos( math.pi/16 ) ] ) )
tr.setTranslation( ( 100, -50, 0 ) )
# get an order 2 resampler for volumes of S16
resp = aims.ResamplerFactory_S16().getResampler( 2 )
resp.setDefaultValue( -1 ) # set background to -1
resp.setRef( vol ) # volume to resample
# resample into a volume of dimension 200x200x200 with voxel size 1.1, 1.1, 1.5
resampled = resp.doit( tr, 200, 200, 200, ( 1.1, 1.1, 1.5 ) )
# Note that the header transformations to external referentials have been updated
print resampled.header()[ 'referentials' ]
print resampled.header()[ 'transformations' ]
aims.write( resampled, 'resampled.nii' )</screen>
        Load the original image and the resampled in Anatomist. See how the resampled has been rotated. Now apply the NIFTI/SPM referential info on both images. They are now aligned again, and cursor clicks correctly go to the same location on both volume, whatever the display referential for each of them.
      </para>
    </sect3>

  </sect1>

  <sect1><title>PyAIMS / PyAnatomist integration</title>

    <sect3><title>Running PyAnatomist with PyAims</title>
      <para>
        When PyAnatomist is run in "direct" mode (library bindings), it is possible to share objects between processing routines in pyaims and viewing models in Anatomist, and interact on them directly and interactively.
      </para>
      <para>
        For an interactive shell with Anatomist rendering enabled, IPython should be run in an appropriate mode for graphical events loop to run. If Anatomist is built using Qt3:
        <screen>ipython -qthread</screen>
        If Anatomist is built using Qt4:
        <screen>ipython -q4thread</screen>
      </para>
      <para>
        Then, in IPython, start Anatomist in direct mode:
        <screen>import anatomist.direct.api as ana
a = ana.Anatomist()</screen>
        Now Anatomist main window is here.
      </para>
    </sect3>

    <sect3><title>Sharing objects between Aims and Anatomist</title>
      <para>
        Objects loaded in pyaims may be wrapped as Anatomist objects:
        <screen>from soma import aims
vol = aims.read( 'data_for_anatomist/subject01/subject01.nii' )
anavol = ana.cpp.AObjectConverter.anatomist( vol )
win = a.createWindow( 'Axial' )
win.addObjects( anavol )</screen>
        Or, in the other way, Anatomist objects may be exported as AIMS objects:
        <screen>anamesh = a.loadObject( 'data_for_anatomist/subject01/subject01_Lwhite.mesh' )
mesh = ana.cpp.AObjectConverter.aims( anamesh )
win.addObjects( anamesh )</screen>
      </para>
    </sect3>

    <sect3><title>Interactive modifications in objects</title>
      <para>
        Objects may be modified on AIMS side. Then Anatomist must be notified of such modifications so that views on the objects can be refreshed.
        <screen>import numpy
arr = numpy.array( vol, copy=False )
arr[ 100:150, 100:150, 50:80 ] += 200
anavol.setChanged() # say this object has changed
anavol.notifyObservers() # refresh views and other interactions</screen>
      </para>
      <para>
        It is also possible to interact on objects created within Anatomist:
        <screen>fusion3d = a.fusionObjects( [ anavol, anamesh ], method='Fusion3DMethod' )
# create a texture object from the fusion3d
a.execute( 'ExtractTexture', object=fusion3d )
# find it in the objects list
anatex = filter( lambda o: o.name.startswith( 'extracted_' ), a.getObjects() )
anatex = anatex[0]
texsurf = a.fusionObjects( [ anamesh, anatex ], method='FusionTexSurfMethod' )
win2 = a.createWindow( '3D' )
win2.addObjects( texsurf )
# now modify the texture
tex = ana.cpp.AObjectConverter.aims( anatex )
        </screen>
      </para>
    </sect3>

  </sect1>

</chapter>


</book>
